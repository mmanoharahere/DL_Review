{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4dd283-4007-4b7b-8d02-c9fb121b1887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.0-cp311-none-macosx_11_0_arm64.whl (59.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, networkx, fsspec, torch\n",
      "Successfully installed fsspec-2024.2.0 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.2.0 typing-extensions-4.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3d1e00d-6808-4af5-8ee3-4c18dd9056a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc61acf-e467-42d2-80d1-144ca5baea74",
   "metadata": {},
   "source": [
    "We are going to be writing ViT architecture from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e85897f-ed47-4d6b-b22e-26975997dc52",
   "metadata": {},
   "source": [
    "We need to have different modules for ViT:\n",
    "#https://towardsdatascience.com/implementing-vision-transformer-vit-from-scratch-3e192c6155f0\n",
    "1. PatchEmbeddings\n",
    "2. Embeddings\n",
    "3. Attention\n",
    "4. MHA\n",
    "5. MLP\n",
    "6. Block\n",
    "7. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b24580c-830d-4d2d-b12c-454877c28085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Converts image to patches and then projects them to vector space.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config.img_size\n",
    "        self.patch_size = config.patch_size\n",
    "        self.input_channels = config.input_channels\n",
    "        self.output_channels = config.hidden_size\n",
    "        self.num_patches = (self.img_size // self.patch_size) **2\n",
    "        self.proj = nn.Conv2d(self.input_channels, self.output_channels, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        ## BS, input_channels, img_h, img_w -> BS, num_patches, output_channels\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c317a8-40c6-4dc9-b9d4-4bca5654d74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,config.hidden_size))\n",
    "        self.pos_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches +1, config.hidden_size))\n",
    "    def forward(self,x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        bs, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(bs, -1, -1)\n",
    "        x = torch.cat((cls_token,x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dae5ba-9c06-4c62-a1fa-1ffc0da3aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_head_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn_head_sie = attn_head_size\n",
    "        self.query = nn.Linear(hidden_size, attn_hidden_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attn_hidden_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attn_hidden_size, bias=bias)\n",
    "    def forward(self,x):\n",
    "        #bs, seq_len, hidden_size -> bs, seq_len, attn_head_size\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        attn_scores =( torch.matmul(query, key.transpose(-1,-2)))/math.sqrt(self.attn_head_size)\n",
    "        attn_probs = nn.functional.softmax(attn_scores,dim=-1)\n",
    "        attn_output = torch.matmul(attn_probs, value)\n",
    "        return attn_output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72e295-8335-4a1f-841b-8b63b52048fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_attn_heads= config.num_attn_heads\n",
    "        self.attn_head_size = self.hidden_size // self.num_attn_heads\n",
    "        self.qkv_bias = config.qkv_bias\n",
    "\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attn_heads):\n",
    "            head = Attention(self.hidden_size, self.attn_head_size, self.qkv_bias)\n",
    "            self.heads.append(head)\n",
    "\n",
    "        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    def forward(self,x):\n",
    "        attn_outputs = [head(x) for head in self.heads]\n",
    "        attn_output = torch.cat([attn_output for attn_output,_ in attn_outputs], dim=-1)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea5bee-b577-4a67-af82-9fad721fb99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.dense1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.act = GELU()\n",
    "        self.dense2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.dense1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6dda35-cc21-41cc-9847-a49dd76f0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        self.attn = MHA(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        ln1_op = self.ln1(x)\n",
    "        attn_out = self.attn(ln1_op)\n",
    "        x = x + attn_op\n",
    "\n",
    "        ln2_op = self.ln2(x)\n",
    "        mlp_out = self.mlp(ln2_op)\n",
    "        x = x+mlp_out\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb87c8f-0093-41d3-bdfd-ea61bdcd733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config.num_blocks):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "    def forward(self,x):\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
